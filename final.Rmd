---
title: 'final: analysis of NBA team stats'
author: "Kai Li"
date: "April 27, 2016"
output: md_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, warning = F)

library(foreign)
library(psych)
library(nFactors)
library(ggplot2)
library(ape) # dendrogram for large sample
library(cluster)
library(MASS) # lda
library(pander)
library(car) #variance score test
library(lmtest) #Durbin-Watson test
library(diptest) #unimodality test
library(nFactors)
library(vegan)
library(dplyr)

#define a helper function (borrowed from the "ez" package)
ezLev=function(x,new_order){
	for(i in rev(new_order)){
		x=relevel(x,ref=i)
	}
	return(x)
}

ggcorplot = function(data,var_text_size,cor_text_limits){
	# normalize data
	for(i in 1:length(data)){
		data[,i]=(data[,i]-mean(data[,i]))/sd(data[,i])
	}
	# obtain new data frame
	z=data.frame()
	i = 1
	j = i
	while(i<=length(data)){
		if(j>length(data)){
			i=i+1
			j=i
		}else{
			x = data[,i]
			y = data[,j]
			temp=as.data.frame(cbind(x,y))
			temp=cbind(temp,names(data)[i],names(data)[j])
			z=rbind(z,temp)
			j=j+1
		}
	}
	names(z)=c('x','y','x_lab','y_lab')
	z$x_lab = ezLev(factor(z$x_lab),names(data))
	z$y_lab = ezLev(factor(z$y_lab),names(data))
	z=z[z$x_lab!=z$y_lab,]
	#obtain correlation values
	z_cor = data.frame()
	i = 1
	j = i
	while(i<=length(data)){
		if(j>length(data)){
			i=i+1
			j=i
		}else{
			x = data[,i]
			y = data[,j]
			x_mid = min(x)+diff(range(x))/2
			y_mid = min(y)+diff(range(y))/2
			this_cor = cor(x,y)
			this_cor.test = cor.test(x,y)
			this_col = ifelse(this_cor.test$p.value<.05,'<.05','>.05')
			this_size = (this_cor)^2
			cor_text = ifelse(
				this_cor>0
				,substr(format(c(this_cor,.123456789),digits=2)[1],2,4)
				,paste('-',substr(format(c(this_cor,.123456789),digits=2)[1],3,5),sep='')
			)
			b=as.data.frame(cor_text)
			b=cbind(b,x_mid,y_mid,this_col,this_size,names(data)[j],names(data)[i])
			z_cor=rbind(z_cor,b)
			j=j+1
		}
	}
	names(z_cor)=c('cor','x_mid','y_mid','p','rsq','x_lab','y_lab')
	z_cor$x_lab = ezLev(factor(z_cor$x_lab),names(data))
	z_cor$y_lab = ezLev(factor(z_cor$y_lab),names(data))
	diag = z_cor[z_cor$x_lab==z_cor$y_lab,]
	z_cor=z_cor[z_cor$x_lab!=z_cor$y_lab,]
	#start creating layers
	points_layer = layer(
		geom = 'point'
		, data = z
		, mapping = aes(
			x = x
			, y = y
		)
	)
	lm_line_layer = layer(
		geom = 'line'
		, geom_params = list(colour = 'red')
		, stat = 'smooth'
		, stat_params = list(method = 'lm')
		, data = z
		, mapping = aes(
			x = x
			, y = y
		)
	)
	lm_ribbon_layer = layer(
		geom = 'ribbon'
		, geom_params = list(fill = 'green', alpha = .5)
		, stat = 'smooth'
		, stat_params = list(method = 'lm')
		, data = z
		, mapping = aes(
			x = x
			, y = y
		)
	)
	cor_text = layer(
		geom = 'text'
		, data = z_cor
		, mapping = aes(
			x=y_mid
			, y=x_mid
			, label=cor
			, size = rsq
			, colour = p
		)
	)
	var_text = layer(
		geom = 'text'
		, geom_params = list(size=var_text_size)
		, data = diag
		, mapping = aes(
			x=y_mid
			, y=x_mid
			, label=x_lab
		)
	)
	f = facet_grid(y_lab~x_lab,scales='free')
	o = opts(
		panel.grid.minor = theme_blank()
		,panel.grid.major = theme_blank()
		,axis.ticks = theme_blank()
		,axis.text.y = theme_blank()
		,axis.text.x = theme_blank()
		,axis.title.y = theme_blank()
		,axis.title.x = theme_blank()
		,legend.position='none'
	)
	size_scale = scale_size(limits = c(0,1),to=cor_text_limits)
	return(
		ggplot()+
		points_layer+
		lm_ribbon_layer+
		lm_line_layer+
		var_text+
		cor_text+
		f+
		o+
		size_scale
	)
}


data <- read.csv("nba_data.csv")

```

# Final project: Regression and MDS analyses on NBA team stats

Kai Li

6/1/2016

## Research questions

1. How well can the existing regular team stats help to predict winning matches of the NBA teams?
2. How can the stats be classified according to the dataset?

## Data description

The dataset that is used in this study is the [NBA team stats dataset](https://raw.githubusercontent.com/tyson-ni/nba/master/NBA_train.csv) uploaded to Github by [Tyson Ni](https://github.com/tyson-ni/). It is not stated where the dataset was originally from. 

This dataset is entitled "NBA_train" and is in CSV format. It includes the basic statistics of all the NBA teams in the regular season games from 1979-1980 to 2010-2011, excluding the shortened season of 1998-1999. Each of the `r nrow(data)` rows is the performance of a team in a given season. `r length(data)` variables are available in the dataset, including:

- **SeasonEnd**: the end year of the season
- **Team**: the name of the team
- **Team.1**: the label of the team
- **Franchise**: the franchise name of the team
- **Playoffs**: whether the team entered playoffs in that season ("1": yes; "0": no)
- **W**: number of games won (There have been 82 games in the regular season since the season of 1967-1968, except for the season of 1998-1999, when there were 50 games, thanks to the lockout.)
- **PTS**: total points made during the season
- **oppPTS**: total points conceded during the season
- **FG**: total field goals made during the season
- **FGA**: total field goals attempted during the season
- **2P**: 2-point goals made during the season
- **2PA**: 2-point goals attempted during the season
- **3P**: 3-point goals made during the season
- **3PA**: 3-point goals attempted during the season
- **FT**: free throws made during the season
- **FTA**: free throws attempted during the season
- **ORB**: total offensive rebounds during the season
- **DRB**: total defensive rebounds during the season
- **AST**: total assists during the season
- **STL**: total steals during the season
- **BLK**: total blocks during the season
- **TOV**: total turnovers during the season

This dataset was manipulated in a few ways. First, some other variables are created based on the existing ones.

``` {R}

## New variables
data.1 <- data.frame(c(1:835))
data.1[,1:6] <- data[,1:6]

for (i in 1:nrow(data)) {
  data.1[,7:22] <- data[,7:22] / 82
}

data.1$FGP <- data.1$FG / data.1$FGA
data.1$X2PP <- data.1$X2P / data.1$X2PA
data.1$X3PP <- data.1$X3P / data.1$X3PA
data.1$FTP <- data.1$FT / data.1$FTA
data.1$absPTS = data.1$PTS - data.1$oppPTS
data.1$X2P_rate <- data.1$X2P / data.1$FG
data.1$X3P_rate <- data.1$X3P / data.1$FG
data.1$X2PA_rate <- data.1$X2PA / data.1$FGA
data.1$X3PA_rate <- data.1$X3PA / data.1$FGA

colnames(data.1)[1] <- "SeasonEnd"

```

- **absPTS**: the difference between points and opponent points
- **FGP**: the percentage of field goals made to field goals attempted
- **X2PP**: the percentage of 2-point goals made to 2-point goals attempted
- **X3PP**: the percentage of 3-point goals made to 3-point goals attempted
- **FTP**: the percentage of free throws made to free throws attempted

Second, all the variables that are total numbers in a season rather than percentages are further divided by 82 to be transformed from per season to per game statistics. 

Third, all the teams are separated into three groups by season:

- Group 1: 1980-1989
- Group 2: 1990-1999
- Group 3: 2000-2011

``` {R}

for (i in 1:nrow(data.1)) {
  if (data.1$SeasonEnd[i] < 1990) {
    data.1$period[i] = 1
  }
  if (data.1$SeasonEnd[i] > 1999) {
    data.1$period[i] = 3
  }
  if (data.1$SeasonEnd[i] < 2000 & data.1$SeasonEnd[i] > 1989) {
    data.1$period[i] = 2
  }
}

##write.csv(data.1, "final.dataset.csv", row.names = F)

```

``` {R}

data.1 <- read.csv("final.dataset.csv")

```

## Analysis

#### Descriptive analysis

Some basic descriptive analysis was conducted. Below are the total numbers of teams in the NBA league and the distribution of the number of wins across the dataset.

``` {R fig.width = 7, fig.height = 4}

plot <- ggplot(data.1) +
  geom_bar(aes(x = SeasonEnd), stat = "count") +
  labs(y = "count of team", title = "Count of NBA teams")
plot

```

``` {R  fig.width = 7, fig.height = 4}

plot(as.factor(data.1$W), main = "Bar chart of team wins", ylab = "Count", xlab = "Wins",
     ylim = c(0, 40))

```

Below is the summary of the number of teams in the three time periods that were created above.

``` {R}

period.table <- data.frame(Group = c("Group 1", "Group 2", "Group 3"),
                           Period = c("1980-1989", "1990-1999", "2000-2011"),
                           Count = c(nrow(data.1[data.1$period == "1",]), nrow(data.1[data.1$period == "2",]), nrow(data.1[data.1$period == "3",])))

pander(period.table)

```

Thanks to the nature of the NBA games, a lot of the variables in the NBA games are highly correlated, as shown below in the correlation matrix based on some numeric variables. It is natural that FG- and X2P-related variables are highly correlated with each other, both of which are pretty strongly correlated with assists. On the other hand, X3P-related variables seem to be the exceptions in this dataset.

``` {R fig.width = 7, fig.height = 6}

cor.matrix <- round(cor(data.1[,c(6, 7, 8, 9, 11, 13, 15, 17:25)]), digits = 2)
cor.plot(cor.matrix)

```

#### Regression to predict the number of winning match

To predict the number of winning match, a regression model is established. Because of the highly correlated nature of the data, variables that are highly correlated are not included in the data.

> model <- lm(W ~ PTS + oppPTS + FG + X2P + ORB + DRB + AST + STL + BLK + TOV + FGP + X2PP + X3PP)

``` {R fig.width = 7, fig.height = 7}

lm <- lm(W ~ PTS + oppPTS + FG + X2P + ORB + DRB + AST + STL + BLK + TOV + FGP + X2PP + X3PP, data = data.1)

par(mfrow = c(2,2))
plot(lm, which = 1:4)

```

According to the diagnose diagram presented above, it seems that the assumptions of regression analysis are largely met, including the following:

- **linearity assumption**: it is met because the dots seems to be relatively evenly distributed across the 0 line in Residuals vs. fitted plot;
- **homoscenasticity assumption**: because the trend lines in both the Residuals vs. fitted plot and Scale-location plot are pretty straight parallel to the x-axis. Moreover, the results of Breusch-Pagan test also suggest that the null hypothesis of homoscedasticity cannot be rejected because of its p-value equals to `r round(ncvTest(lm)$p, digits = 4)`.
- **normality assumption**: all the dots are plotted well in the QQ plot
- **influential outliers**: based on Cook's distance plot, no observation is an influential outliers given their relatively low values.

Moreover, **independence assumption** is tested using Durbin-Watson test. However, based on its result, the null hypothesis that the observations are independent can be rejected at 0.05 level. The problem might be that I have selected a number of variables that are so strongly correlated with each other.

``` {R}

dwtest(lm)

```

Last, **multicollinearity** is tested using variance inflation factors. It seems that a number of independent variables violate this assumption given their strong collinearity. Based on the result and the correlation matrix above, **PTS**, **oppPTS**, **FG**, **X2P** (correlated with PTS), and **FGP** (correlated with X2PP) are removed from the model.

``` {R}

vif(lm)

```

Based on the results from the assumption tests, the following model is proposed and tested against the assumptions.

> model.1 <- glm(W ~ ORB + DRB + AST + STL + BLK + TOV + X2PP + X3PP)

``` {R fig.width = 7, fig.height = 6}

cor.matrix.2 <- round(cor(data.1[,c(6, 17, 18, 19, 20, 21, 22, 24, 25)]), digits = 2)
cor.plot(cor.matrix.2, numbers = T, cex = 0.7)

```

Below are the results of the VIF test.

``` {R}

lm.1 <- lm(W ~ ORB + DRB + AST + STL + BLK + TOV + X2PP + X3PP,
            data = data.1)

vif(lm.1)

dwtest(lm.1)

```

According to the result, this model is able to interpret about 64% of the variance in the dataset.

``` {R}

summary(lm.1)

```

The model is used to predict the winning matches. The differences between real winning match and predicted winning match are calculated. According to the histogram, even though half of the instances are within 2 match difference, there are also some cases where the difference is as large as 30 matches.

``` {R}

data.2 <- data.1[,c(6, 17, 18, 19, 20, 21, 22, 24, 25, 2)]

data.2$W_predicted <- coef(lm.1)[1] + data.2[,2] * coef(lm.1)[2] + data.2[,3] * coef(lm.1)[3] + data.2[,4] * coef(lm.1)[4] + data.2[,5] * coef(lm.1)[5] + data.2[,6] * coef(lm.1)[6] + data.2[,7] * coef(lm.1)[7] + data.2[,8] * coef(lm.1)[8] + data.2[,9] * coef(lm.1)[9]
data.2$W_predicted <- round(data.2$W_predicted, digits = 0)
data.2$difference <- data.2$W - data.2$W_predicted

different.2 <- sum(abs(data.2$difference)) / 835

```

``` {R fig.width = 7, fig.height = 4}

hist(data.2$difference, 
     ylim = c(0, 60), 
     xlab = "Winning match difference", 
     breaks = 40,
     main = "Difference between reality and prediction")

```

###### Another approach:

Function *stepAIC* from the package MASS could automatically calculate the subset of a model with the smallest AIC value. As a result, I used my initial model as the starting point:

> model <- lm(W ~ PTS + oppPTS + FG + X2P + ORB + DRB + AST + STL + BLK + TOV + FGP + X2PP + X3PP)

``` {R}

step <- stepAIC(lm, direction = "both")

```

The suggested model by this function is presented below. Based on the results, it seems that this model has a much better predictive performance than the original model.

> model.3 <- lm(W ~ PTS + oppPTS + DRB + BLK + TOV + X2PP + X3PP)

``` {R}

lm.3 <- lm(W ~ PTS + oppPTS + DRB + BLK + TOV + X2PP + X3PP,
            data = data.1)

summary(lm.3)

```

However, one of the problems is that this model, by including some highly correlated variables, fails to meet the requirement of multicollinearity as shown below.

``` {R}

vif(lm.3)

```

However, as suggested by the histogram and table below, the new model is a big improvement compared with the original model, despite of its violation of the assumptions.

``` {R}

data.3 <- data.1[,c(6, 7, 8, 18, 21, 22, 24, 25, 2)]

data.3$W_predicted <- coef(lm.3)[1] + data.3[,2] * coef(lm.3)[2] + data.3[,3] * coef(lm.3)[3] + data.3[,4] * coef(lm.3)[4] + data.3[,5] * coef(lm.3)[5] + data.3[,6] * coef(lm.3)[6] + data.3[,7] * coef(lm.3)[7] + data.3[,8] * coef(lm.3)[8]
data.3$W_predicted <- round(data.3$W_predicted, digits = 0)
data.3$difference <- data.3$W - data.3$W_predicted

different.3 <- sum(abs(data.3$difference)) / 835

```

``` {R fig.width = 7, fig.height = 4}

hist(data.3$difference, 
     ylim = c(0, 120), 
     breaks = 20,
     main = "Histogram of difference by new model")

```

Moreover, the mean of the match difference using the new model is also significantly smaller than the other model.

``` {R}

difference_table <- data.frame(Model = c("Original", "New"),
                               Mean_absolute_difference = (c(different.2, different.3)))
pander(difference_table)

```

## MDS analysis on measurement classification

To answer the second question, only the variables in the original model presented in the first section are selected, with two extra variables, X2P and X3P.

``` {R}

data.2 <- subset(data.1, data.1$period == 1)
data.3 <- subset(data.1, data.1$period == 2)
data.4 <- subset(data.1, data.1$period == 3)

cor.matrix.1 <- round(cor(data.1[,c(6, 11, 13, 17, 18, 19, 20, 21, 22, 24, 25)]), digits = 2)
cor.matrix.2 <- round(cor(data.2[,c(6, 11, 13, 17, 18, 19, 20, 21, 22, 24, 25)]), digits = 2)
cor.matrix.3 <- round(cor(data.3[,c(6, 11, 13, 17, 18, 19, 20, 21, 22, 24, 25)]), digits = 2)
cor.matrix.4 <- round(cor(data.4[,c(6, 11, 13, 17, 18, 19, 20, 21, 22, 24, 25)]), digits = 2)
cor.matrix.1

```

Below is the heatmap of the variables. 

``` {R}

heatmap(cor.matrix.1)

```

Given the interval/ratio nature of the dataset, classical MDS method is used to analyze the dataThe MDS plots of all time and the three time periods are presentede below, all of which use 2-dimension solutions:

``` {R fig.width = 7, fig.height = 8}

fit_cla.1 <- cmdscale(dist(cor.matrix.1), k = 2, eig = T)
fit_cla.2 <- cmdscale(dist(cor.matrix.2), k = 2, eig = T)
fit_cla.3 <- cmdscale(dist(cor.matrix.3), k = 2, eig = T)
fit_cla.4 <- cmdscale(dist(cor.matrix.4), k = 2, eig = T)

par(mfrow = c(2,2))

x.1 <- as.data.frame(fit_cla.1$points[,1])
y.1 <- as.data.frame(fit_cla.1$points[,2])
x.2 <- as.data.frame(fit_cla.2$points[,1])
y.2 <- as.data.frame(fit_cla.2$points[,2])
x.3 <- as.data.frame(fit_cla.3$points[,1])
y.3 <- as.data.frame(fit_cla.3$points[,2])
x.4 <- as.data.frame(fit_cla.4$points[,1])
y.4 <- as.data.frame(fit_cla.4$points[,2])

y.2[,1] <- 0-as.numeric(y.2[,])
x.1[,1] <- 0-as.numeric(x.1[,])
x.4[,1] <- 0-as.numeric(x.4[,])

cluster.data <- data.frame(Measure = rownames(x.1), 
                           label = rep(c("W", "X2P", "X3P", "ORB", "DRB", "AST", "STL", "BLK", "TOV", "X2PP", "X3PP"),
                                       times = 4),
                           group = c(rep("all", times = 11), rep("period 1", 11), rep("period 2", 11), rep("period 3", 11)),
                           x = c(x.1[,1], x.2[,1], x.3[,1], x.4[,1]),
                           y = c(y.1[,1], y.2[,1], y.3[,1], y.4[,1]))

plot <- ggplot(cluster.data, aes(x = x, y = y, colour = group, label = label)) +
  geom_text(size = 3) +
  facet_wrap(~group)
plot

```

Below is the evaluation of the goodness of fit of the four fittings. The results indicate that this model is not so much well fitted to the three subsets using a two-dimensional solution. 

``` {R}

gof.table <- data.frame(Model = c("All", "Period 1", "Period 2", "Period 3"),
                        GOF = c(round(fit_cla.1$GOF[1], digits = 3),
                                round(fit_cla.2$GOF[1], digits = 3),
                                round(fit_cla.3$GOF[1], digits = 3),
                                round(fit_cla.4$GOF[1], digits = 3)))

pander(gof.table)

```

During the past 30 years, there has been not only a great increase in the total number of 3-point goals made and the 3-point goal percentage, as shown in the illustration below. However, 3-point goal related variables are still highly different from other measurements, even during the past 10 years.

``` {R}

plot <- ggplot(data.1) +
  geom_point(aes(x = X3P, y = X3PP)) +
  facet_wrap(~period)
plot

```

Moreover, the pattern of measurements has been pretty consistent during the three decades. Moreover, 2-point goal percentage, defensive rebound, and assist are the variables that are the most close to winning matches. 


